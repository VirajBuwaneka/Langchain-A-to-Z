{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "282918c5",
   "metadata": {},
   "source": [
    "## Conversational RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1cd714a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n",
      "WARNING: There was an error checking the latest version of pip.\n",
      "ERROR: Could not find a version that satisfies the requirement langchain-chromadb (from versions: none)\n",
      "ERROR: No matching distribution found for langchain-chromadb\n",
      "WARNING: There was an error checking the latest version of pip.\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "#installing the necessary packages\n",
    "\n",
    "!pip install langchain -qU\n",
    "!pip install openai -qU\n",
    "!pip install langchain-chromadb -qU\n",
    "!pip install langchain_community -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16bb0f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\msi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\msi\\appdata\\roaming\\python\\python310\\site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\msi\\appdata\\roaming\\python\\python310\\site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (0.11.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\msi\\appdata\\roaming\\python\\python310\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\msi\\appdata\\roaming\\python\\python310\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\msi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\msi\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\msi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: certifi in c:\\users\\msi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\msi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\msi\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\msi\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\msi\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "047ded13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Access your API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Example request\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, OpenAI!\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5be95cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msi\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57888829",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eef54f3",
   "metadata": {},
   "source": [
    "### Initialize the Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9657619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embedding_model= OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb24127c",
   "metadata": {},
   "source": [
    "### Load PDF Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b953c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf -qU\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8fec271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load the PDF document\n",
    "loader = PyPDFLoader(\"Proposal.pdf\")\n",
    "\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdee1059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64894bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8. Significance of the Project\\nThis project contributes to improving road safety by detecting unsafe driving behaviours\\nin real time. Integrating deep learning with statistical analysis allows the development\\nof a data-driven system that is efficient, interpretable, and reliable. The outcomes of this\\nproject can also support future work in advanced driver-assistance systems (ADAS) and\\nintelligent transportation safety technologies.\\n9. Expected Challenges\\nâ€¢Limited dataset or class imbalance.\\nâ€¢Difficulty in distinguishing similar facial postures.\\nâ€¢Ensuring reliable performance under different lighting or camera angles.\\nâ€¢Optimizing the model for real-time performance.\\n10. Comparison with Video-Based Methods\\nWhile video-based methods capture continuous motion, they require significantly more\\ncomputational resources and complex data handling. The proposed image-based method\\noffers several advantages:\\nâ€¢Lower computational cost and faster processing.\\nâ€¢Simpler dataset preparation and annotation.\\nâ€¢Efficient for real-time detection using snapshots.\\nâ€¢Easier to deploy on low-cost hardware systems.\\nAlthough image-based detection focuses on static frames, it is sufficient for identifying\\nfacial behaviours like sleeping or eating. Once the model achieves strong performance, it\\ncan later be extended into a video-based approach using CNN-LSTM or 3D architectures.\\n11. Future Enhancements\\nâ€¢Extend to real-time video-based detection for continuous monitoring.\\nâ€¢Add more behaviour categories such as phone usage or yawning.\\n4'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[3].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45738742",
   "metadata": {},
   "source": [
    "### Split Documents in to Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58511661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "splits=text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5dbe0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc6a95",
   "metadata": {},
   "source": [
    "### Create Vector Store and Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1177f8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.vectorstores'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain.vectorstores'"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b270d6d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.vectorstores'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Create a vector store from the document chunks\u001b[39;00m\n\u001b[0;32m      6\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(documents\u001b[38;5;241m=\u001b[39msplits, embedding\u001b[38;5;241m=\u001b[39membedding_model)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain.vectorstores'"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "\n",
    "# Create a vector store from the document chunks\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1fa2dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d815c8b",
   "metadata": {},
   "source": [
    "### Define prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "913ab3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define the system prompt\n",
    "system_prompt = (\n",
    "    \"You are an intelligent chatbot. Use the following context to answer the question. If you don't know the answer, just say that you don't know.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Create the prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "592d083a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an intelligent chatbot. Use the following context to answer the question. If you don't know the answer, just say that you don't know.\\n\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bd260e",
   "metadata": {},
   "source": [
    "### Create RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3d2cc82",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.chains'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_retrieval_chain\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombine_documents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_stuff_documents_chain\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Create the question-answering chain\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.chains'"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Create the question-answering chain\n",
    "qa_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_chain = create_retrieval_chain(retriever, qa_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45170324",
   "metadata": {},
   "source": [
    "### Invoke RAG chain With Example Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385ecdc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but based on the provided context, I don't have information about RAG architecture. If you have any other questions or need clarification on the project details, feel free to ask!\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what is RAG architecture?\"})\n",
    "response[\"answer\"]\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d1f444",
   "metadata": {},
   "source": [
    "### Add Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f8b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# Define the contextualize system prompt\n",
    "contextualize_system_prompt = (\n",
    "    \"using chat history and the latest user question, just reformulate question if needed and otherwise return it as is\"\n",
    ")\n",
    "\n",
    "# Create the contextualize prompt template\n",
    "contextualize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the history-aware retriever\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_prompt\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e76bdd1",
   "metadata": {},
   "source": [
    "### Create History Aware RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a8ea34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001B11A6CB4C0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an intelligent chatbot. Use the following context to answer the question. If you don't know the answer, just say that you don't know.And also use emojies when needed\\n\\n{context}\"), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an intelligent chatbot. Use the following context to answer the question. If you don't know the answer, just say that you don't know.And also use emojies when needed\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d4452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the question-answering chain\n",
    "qa_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Create the history aware RAG chain\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, qa_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915055d6",
   "metadata": {},
   "source": [
    "### Manage Chat Session History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faabc27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# Initialize the store for session histories\n",
    "store = {}\n",
    "\n",
    "# Function to get the session history for a given session ID\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Create the conversational RAG chain with session history\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aede9ec4",
   "metadata": {},
   "source": [
    "### Invoke Conversational RAG Chain With Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e800ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'m sorry, but based on the context provided, there is no information about a person or entity named \"codeprolk.\" If you have any other questions or need assistance with the information provided, feel free to ask! ðŸ˜Š'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "response = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"who is codeprolk\"},\n",
    "    config={\"configurable\": {\"session_id\": \"101\"}},\n",
    ")\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22787f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'101': InMemoryChatMessageHistory(messages=[HumanMessage(content='who is codeprolk', additional_kwargs={}, response_metadata={}), AIMessage(content='I\\'m sorry, but based on the context provided, there is no information about a person or entity named \"codeprolk.\" If you have any other questions or need assistance with the information provided, feel free to ask! ðŸ˜Š', additional_kwargs={}, response_metadata={})])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129a7f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'m sorry, but based on the context provided, there is no specific mention of \"RAG architecture.\" If you have more context or details about what RAG architecture refers to, I can try to provide a more accurate answer. Otherwise, I don\\'t have information on that specific term. Let me know if you have any other questions! ðŸ˜Š'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"what is rag architecture\"},\n",
    "    config={\"configurable\": {\"session_id\": \"101\"}},\n",
    ")\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139601a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the context provided, the Deep Learning techniques used in the project for Driver Facial Behaviour Analysis include:\\n\\n1. Using deep learning techniques to extract facial features and classify images into behavior categories.\\n2. Splitting the dataset into training, validation, and testing sets.\\n3. Optimizing hyperparameters to achieve high accuracy and reduce misclassifications.\\n\\nThese techniques involve leveraging deep neural networks to analyze and classify facial behavior data for driver monitoring and safety enhancement. If you have any more questions or need further clarification, feel free to ask! ðŸ˜Š'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"what are the Deep Learning techinques used in this?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"101\"}},\n",
    ")\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde4674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Certainly! Here is a list of Deep Learning techniques used in the Driver Facial Behaviour Analysis project:\\n\\n1. Convolutional Neural Networks (CNNs): Utilized for image-based data processing and feature extraction from facial images.\\n2. Recurrent Neural Networks (RNNs): Potentially used for analyzing sequential data, such as tracking eye movements over time.\\n3. Transfer Learning: Leveraged to fine-tune pre-trained models for facial behavior classification tasks.\\n4. Hyperparameter Optimization: Adjusting parameters like learning rate, batch size, and network architecture to improve model performance.\\n5. Data Augmentation: Increasing the diversity of the training data by applying transformations like rotation, scaling, and flipping to improve model generalization.\\n\\nThese techniques collectively contribute to the development of an effective deep learning model for identifying unsafe driver behaviors through facial analysis. If you have any more questions or need additional details, feel free to ask! ðŸ˜Š'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"can you list down\"},\n",
    "    config={\"configurable\": {\"session_id\": \"101\"}},\n",
    ")\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb8a1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vector store saved!\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"vector_store\"  # âœ… this automatically persists in Chroma 1.1.1\n",
    ")\n",
    "\n",
    "print(\"âœ… Vector store saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a960ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msi\\AppData\\Local\\Temp\\ipykernel_18204\\2617956255.py:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(\n"
     ]
    }
   ],
   "source": [
    "vectordb = Chroma(\n",
    "    persist_directory=\"vector_store\",\n",
    "    embedding_function=embedding_model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4f83ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
